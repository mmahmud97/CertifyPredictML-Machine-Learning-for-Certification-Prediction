{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_10700/515489967.py:4: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cert_active_df = pd.read_csv(\"Cert_AllActive.csv\")\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_10700/515489967.py:5: DtypeWarning: Columns (13,48,56,64,69,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds_customers_df = pd.read_csv(\"DS_Customers.csv\")\n"
     ]
    }
   ],
   "source": [
    "#Cert Active with DS_Customer\n",
    "import pandas as pd\n",
    "\n",
    "cert_active_df = pd.read_csv(\"Cert_AllActive.csv\")\n",
    "ds_customers_df = pd.read_csv(\"DS_Customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Customer_ID', 'cert_type', 'cert_start_date', 'cert_end_date',\n",
       "       'Stadard_Job_Title', 'job_function', 'job_title_freetext',\n",
       "       'companyname', 'email', 'DISALLOW_ALL_COMMUNICATION',\n",
       "       'disallow_email_communication', 'disallow_phone_communication',\n",
       "       'disallow_regular_mail_communi', 'disallow_third_party_communic',\n",
       "       'membership_paid_through', 'employment_status', 'department_size',\n",
       "       'gender', 'ethnicity', 'education', 'company_size',\n",
       "       'SHRM_ENTERED_PROFESSION', 'shrm_birth_year', 'native_language',\n",
       "       'organization_unit', 'organization_type', 'employee_oversee',\n",
       "       'supervisor_title', 'shrm_organization_multination',\n",
       "       'shrm_organization_unionized', 'SHRM_Join_Date', 'Subsidiary',\n",
       "       'billaddr_1', 'billaddr_2', 'billaddr_3', 'billaddr_city',\n",
       "       'billaddr_state', 'billaddr_zip', 'billaddr_country',\n",
       "       'billaddr_country_desc', 'Current_Start_Date', 'Is_Person',\n",
       "       'IsInActive', 'LoginAccess', 'Monthly_Donor', 'Industry_Category',\n",
       "       'SSO_UUID', 'Customer_Segment', 'Cohort', 'ContactId', 'cobalt_name',\n",
       "       'Address1_Country', 'cobalt_expirationdate', 'cobalt_enddate',\n",
       "       'Status Reason', 'Address1_Line1', 'Address1_City',\n",
       "       'Address1_PostalCode', 'Address1_StateOrProvince', 'Address1_Composite',\n",
       "       'EMailAddress1', 'BirthDate', 'shrm_startdate',\n",
       "       'shrm_InitialCertificationDate', 'shrmadm_certificationbegindate',\n",
       "       'shrmadm_certificationenddate', 'shrm_SHRMMember',\n",
       "       'shrm_currentbusinessindustry', 'shrm_currentbusinessindustryvalue',\n",
       "       'shrm_currentpositiontitle', 'shrm_JobPosition',\n",
       "       'shrm_JobPositionvalue', 'shrm_TimesRecertified',\n",
       "       'shrm_TimesRecertified_Date', 'shrm_TimesRecertified_State',\n",
       "       'shrm_ssouid', 'SHRM_SSOEMail', 'targetCertActive', 'Email'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cert Active with DS_Customer\n",
    "import pandas as pd\n",
    "\n",
    "columns_ds_customers = [\n",
    "    'Customer_ID', 'cert_type', 'cert_start_date', 'cert_end_date',\n",
    "    'Stadard_Job_Title', 'job_function', 'job_title_freetext', 'companyname',\n",
    "    'email', 'DISALLOW_ALL_COMMUNICATION', 'disallow_email_communication',\n",
    "    'disallow_phone_communication', 'disallow_regular_mail_communi',\n",
    "    'disallow_third_party_communic', 'membership_paid_through', 'employment_status',\n",
    "    'department_size', 'gender', 'ethnicity', 'education', 'company_size',\n",
    "    'SHRM_ENTERED_PROFESSION', 'shrm_birth_year', 'native_language', \n",
    "    'organization_unit', 'organization_type', 'employee_oversee', 'supervisor_title',\n",
    "    'shrm_organization_multination', 'shrm_organization_unionized', 'SHRM_Join_Date',\n",
    "    'Subsidiary', 'billaddr_1', 'billaddr_2', 'billaddr_3', 'billaddr_city',\n",
    "    'billaddr_state', 'billaddr_zip', 'billaddr_country', 'billaddr_country_desc',\n",
    "    'Current_Start_Date', 'Is_Person', 'IsInActive', 'LoginAccess', 'Monthly_Donor',\n",
    "    'Industry_Category', 'SSO_UUID', 'Customer_Segment', 'Cohort'\n",
    "]\n",
    "\n",
    "columns_cert_active = [\n",
    "    'ContactId', 'cobalt_name', 'Address1_Country', 'cobalt_expirationdate',\n",
    "    'cobalt_enddate', 'Status Reason', 'Address1_Line1', 'Address1_City',\n",
    "    'Address1_PostalCode', 'Address1_StateOrProvince', 'Address1_Composite',\n",
    "    'EMailAddress1', 'BirthDate', 'shrm_startdate', 'shrm_InitialCertificationDate',\n",
    "    'shrmadm_certificationbegindate', 'shrmadm_certificationenddate', 'shrm_SHRMMember',\n",
    "    'shrm_currentbusinessindustry', 'shrm_currentbusinessindustryvalue',\n",
    "    'shrm_currentpositiontitle', 'shrm_JobPosition', 'shrm_JobPositionvalue',\n",
    "    'shrm_TimesRecertified', 'shrm_TimesRecertified_Date', 'shrm_TimesRecertified_State',\n",
    "    'shrm_ssouid', 'SHRM_SSOEMail'\n",
    "]\n",
    "\n",
    "ds_customers_df = ds_customers_df[columns_ds_customers]\n",
    "cert_active_df = cert_active_df[columns_cert_active]\n",
    "\n",
    "# Perform the left join\n",
    "result_df = pd.merge(ds_customers_df, cert_active_df,\n",
    "                     left_on='email', right_on='EMailAddress1', how='left')\n",
    "\n",
    "result_df['targetCertActive'] = result_df['EMailAddress1'].notna().replace({True: 'Yes', False: 'No'})\n",
    "\n",
    "removal_df = pd.read_excel(\"/Users/muzammil.mahmud/Desktop/VSCode/Certification_Dataset/PropensityModel/DS_Transactions_Removal_IDs.xlsx\")\n",
    "\n",
    "#splitting based on yes and no values\n",
    "df_no = result_df[result_df['targetCertActive'] == 'No']\n",
    "df_yes = result_df[result_df['targetCertActive'] == 'Yes']\n",
    "\n",
    "#merging back after removing customer ids that did not exist in DS_Transactions - we could keep this as training, and for testing we could include the ones that were removed because\n",
    "#they did not exist in DS_Transactions - '''IMPORTANTTTTT!!!!!!!!!!!!!!!!!'''\n",
    "reduced_df_no = pd.merge(df_no, removal_df,\n",
    "                     left_on='email', right_on='Email',  # Simplified as the column name is the same in both DataFrames\n",
    "                     how='inner')  # Changed from 'left' to 'inner' to ensure only overlapping IDs are included\n",
    "final_df = pd.concat([reduced_df_no, df_yes], ignore_index=True)\n",
    "\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in each column:\n",
      "Customer_ID                     0.000000\n",
      "cert_type                      82.009620\n",
      "cert_start_date                82.007844\n",
      "cert_end_date                  82.007844\n",
      "Stadard_Job_Title              13.165988\n",
      "                                 ...    \n",
      "shrm_TimesRecertified_State    81.947756\n",
      "shrm_ssouid                    81.951170\n",
      "SHRM_SSOEMail                  81.947756\n",
      "targetCertActive                0.000000\n",
      "Email                          18.052244\n",
      "Length: 79, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of missing values in each column\n",
    "missing_data = final_df.isnull().mean() * 100\n",
    "print(\"Percentage of missing values in each column:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132189\n"
     ]
    }
   ],
   "source": [
    "yes_count = final_df['targetCertActive'].value_counts().get('Yes', 0)\n",
    "print(yes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/4286885342.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df['cert_type'].fillna('No_Type', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#cert type imputation\n",
    "#add place holder value for null\n",
    "\n",
    "# Impute missing seniority_level with a placeholder\n",
    "final_df['cert_type'].fillna('No_Type', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/478340932.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  final_df['cert_start_date'] = pd.to_datetime(final_df['cert_start_date'])\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/478340932.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df['days_since_cert_start_date'].fillna(36500, inplace=True)\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/478340932.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  final_df['cert_end_date'] = pd.to_datetime(final_df['cert_end_date'])\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/478340932.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df['days_since_cert_start_date'].fillna(36500, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#cert_start_date, cert_end_date columns feature engineering\n",
    "\n",
    "final_df['cert_start_date'] = pd.to_datetime(final_df['cert_start_date'])\n",
    "\n",
    "# Reference date for calculating days since the event (example: today)\n",
    "reference_date = pd.to_datetime('2024-01-01')\n",
    "\n",
    "# Days since the event for those who attended\n",
    "final_df['days_since_cert_start_date'] = (reference_date - final_df['cert_start_date']).dt.days\n",
    "\n",
    "# Fill missing values for non-attendees with a large constant\n",
    "final_df['days_since_cert_start_date'].fillna(36500, inplace=True)\n",
    "\n",
    "# Additional temporal features (optional)\n",
    "final_df['cert_start_date_month'] = final_df['cert_start_date'].dt.month.fillna(0).astype(int)  # 0 for non-attendees\n",
    "final_df['cert_start_date_weekday'] = final_df['cert_start_date'].dt.weekday.fillna(0).astype(int)  # 0 for non-attendees\n",
    "\n",
    "\n",
    "final_df['cert_end_date'] = pd.to_datetime(final_df['cert_end_date'])\n",
    "# Reference date for calculating days since the event (example: today)\n",
    "reference_date = pd.to_datetime('2024-01-01')\n",
    "\n",
    "# Days since the event for those who attended\n",
    "final_df['days_since_cert_start_date'] = (reference_date - final_df['cert_start_date']).dt.days\n",
    "\n",
    "# Fill missing values for non-attendees with a large constant\n",
    "final_df['days_since_cert_start_date'].fillna(36500, inplace=True)\n",
    "\n",
    "# Additional temporal features (optional)\n",
    "final_df['cert_start_date_month'] = final_df['cert_start_date'].dt.month.fillna(0).astype(int)  # 0 for non-attendees\n",
    "final_df['cert_start_date_weekday'] = final_df['cert_start_date'].dt.weekday.fillna(0).astype(int)  # 0 for non-attendees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Stadard_Job_Title' after custom imputation: 85735\n",
      "Missing values in 'Stadard_Job_Title' after full imputation: 0\n"
     ]
    }
   ],
   "source": [
    "#Initial Imputation for Stadard Job Title\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Function to fill missing Stadard_Job_Title using job_function and shrm_JobPositionvalue\n",
    "def fill_job_title(row):\n",
    "    if pd.isna(row['Stadard_Job_Title']):\n",
    "        if pd.notna(row['job_function']):\n",
    "            return row['job_function']\n",
    "        elif pd.notna(row['shrm_JobPositionvalue']):\n",
    "            return row['shrm_JobPositionvalue']\n",
    "    return row['Stadard_Job_Title']\n",
    "\n",
    "final_df['Stadard_Job_Title'] = final_df.apply(fill_job_title, axis=1)\n",
    "print(\"Missing values in 'Stadard_Job_Title' after custom imputation:\", final_df['Stadard_Job_Title'].isnull().sum())\n",
    "\n",
    "#Simple Imputation step 2\n",
    "# If there are still missing values, use a simple imputer (e.g., most frequent)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "# Note the addition of [:, 0] to correctly shape the array for the DataFrame column\n",
    "final_df['Stadard_Job_Title'] = imputer.fit_transform(final_df[['Stadard_Job_Title']])[:, 0]\n",
    "\n",
    "# Verify all missing values are filled\n",
    "print(\"Missing values in 'Stadard_Job_Title' after full imputation:\", final_df['Stadard_Job_Title'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of imputed 'company names' as percentages:\n",
      "The Boeing Company             67.001549\n",
      "Retired                         4.098323\n",
      "Self Employed                   3.346546\n",
      "ADP                             1.701750\n",
      "Paychex/ Inc.                   1.272325\n",
      "                                 ...    \n",
      "Air Force Aid Society Inc       0.001139\n",
      "Benhaven Inc                    0.001139\n",
      "ABM Industries Incorporated     0.001139\n",
      "APEX BENEFITS GROUP/ INC.       0.001139\n",
      "AFB International               0.001139\n",
      "Name: proportion, Length: 1327, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Intelligent imputation for 'companyname'\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame\n",
    "# First, fill other categorical variables with a placeholder to stabilize the base\n",
    "categorical_vars = ['company_size', 'organization_type', 'department_size', 'organization_unit']\n",
    "\n",
    "# Use SimpleImputer to fill missing values with the most frequent category\n",
    "imputer = SimpleImputer(strategy='most_frequent') #--could be replaced with new category\n",
    "final_df[categorical_vars] = imputer.fit_transform(final_df[categorical_vars])\n",
    "\n",
    "# Encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in categorical_vars:\n",
    "    final_df[column] = le.fit_transform(final_df[column])\n",
    "\n",
    "# Preparing the feature columns and the target column\n",
    "features = final_df[categorical_vars]\n",
    "target = final_df['companyname']\n",
    "\n",
    "# Encode the target 'companyname'\n",
    "target_filled = target.dropna()\n",
    "le_company = LabelEncoder()\n",
    "target_filled_encoded = le_company.fit_transform(target_filled)\n",
    "\n",
    "# Splitting data that has 'companyname' available into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.loc[target_filled.index], target_filled_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting missing 'companyname'\n",
    "missing_indexes = final_df[target.isnull()].index\n",
    "predicted_names_encoded = clf.predict(features.loc[missing_indexes])\n",
    "predicted_names = le_company.inverse_transform(predicted_names_encoded)\n",
    "\n",
    "# Filling in the missing 'companyname' in the original DataFrame\n",
    "final_df.loc[missing_indexes, 'companyname'] = predicted_names\n",
    "\n",
    "# Calculate and print the percentage distribution of the imputed company names\n",
    "imputed_distribution = pd.Series(predicted_names).value_counts(normalize=True) * 100\n",
    "print(\"Distribution of imputed 'company names' as percentages:\")\n",
    "print(imputed_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of imputed 'employee_oversee' as percentages:\n",
      "0              95.718204\n",
      "2-4             3.803001\n",
      "10-24           0.160466\n",
      "5-9             0.157601\n",
      "25-49           0.137543\n",
      "1               0.012243\n",
      "100-499         0.009378\n",
      "50-99           0.001302\n",
      "500 or more     0.000260\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Emplyee Oversee Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fill missing values in categorical predictors\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "final_df[categorical_vars] = imputer.fit_transform(final_df[categorical_vars])\n",
    "\n",
    "# Assuming categorical_vars are previously defined and include relevant predictors\n",
    "features = final_df[categorical_vars]\n",
    "features = pd.get_dummies(features)  # Convert categorical variables to dummy/indicator variables\n",
    "\n",
    "# Encode and split the target variable 'employee_oversee'\n",
    "target = final_df['employee_oversee'].dropna()\n",
    "le = LabelEncoder()\n",
    "target_encoded = le.fit_transform(target)\n",
    "\n",
    "# Only select rows where 'employee_oversee' is not missing\n",
    "known_indexes = final_df[~final_df['employee_oversee'].isnull()].index\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.loc[known_indexes], target_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the missing values\n",
    "missing_indexes = final_df[final_df['employee_oversee'].isnull()].index\n",
    "if len(missing_indexes) > 0:\n",
    "    predicted_values_encoded = clf.predict(features.loc[missing_indexes])\n",
    "    predicted_values = le.inverse_transform(predicted_values_encoded)\n",
    "\n",
    "    # Fill in the missing 'employee_oversee' in the original DataFrame\n",
    "    final_df.loc[missing_indexes, 'employee_oversee'] = predicted_values\n",
    "\n",
    "    # Output distribution\n",
    "    imputed_distribution = pd.Series(predicted_values).value_counts(normalize=True) * 100\n",
    "    print(\"Distribution of imputed 'employee_oversee' as percentages:\")\n",
    "    print(imputed_distribution)\n",
    "else:\n",
    "    print(\"No missing 'employee_oversee' to impute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of imputed 'supervisor_title' as percentages:\n",
      "President/ CEO/ Chairman                  75.264072\n",
      "Director of HR                            23.535050\n",
      "HR Manager                                 0.948117\n",
      "Other                                      0.154494\n",
      "I'm Highest Ranked In My Org               0.051409\n",
      "Administrator                              0.020617\n",
      "VP of HR                                   0.014191\n",
      "Chief of Staff/ Div Head/ Dept Chair       0.002410\n",
      "Partner/ Principal/ Executive Director     0.002142\n",
      "Dean/ Superintendent/ General Manager      0.002142\n",
      "CHRO/ CHCO                                 0.001874\n",
      "Exec Officer (Govt/Military)               0.001339\n",
      "CFO                                        0.001071\n",
      "SVP of HR or other SVP                     0.000536\n",
      "COO                                        0.000536\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Supervisor Title Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fill missing values in categorical predictors\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "final_df[categorical_vars] = imputer.fit_transform(final_df[categorical_vars])\n",
    "\n",
    "# Assuming categorical_vars are previously defined and include relevant predictors\n",
    "features = final_df[categorical_vars]\n",
    "features = pd.get_dummies(features)  # Convert categorical variables to dummy/indicator variables\n",
    "\n",
    "# Encode and split the target variable 'employee_oversee'\n",
    "target = final_df['supervisor_title'].dropna()\n",
    "le = LabelEncoder()\n",
    "target_encoded = le.fit_transform(target)\n",
    "\n",
    "# Only select rows where 'employee_oversee' is not missing\n",
    "known_indexes = final_df[~final_df['supervisor_title'].isnull()].index\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.loc[known_indexes], target_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the missing values\n",
    "missing_indexes = final_df[final_df['supervisor_title'].isnull()].index\n",
    "if len(missing_indexes) > 0:\n",
    "    predicted_values_encoded = clf.predict(features.loc[missing_indexes])\n",
    "    predicted_values = le.inverse_transform(predicted_values_encoded)\n",
    "\n",
    "    # Fill in the missing 'employee_oversee' in the original DataFrame\n",
    "    final_df.loc[missing_indexes, 'supervisor_title'] = predicted_values\n",
    "\n",
    "    # Output distribution\n",
    "    imputed_distribution = pd.Series(predicted_values).value_counts(normalize=True) * 100\n",
    "    print(\"Distribution of imputed 'supervisor_title' as percentages:\")\n",
    "    print(imputed_distribution)\n",
    "else:\n",
    "    print(\"No missing 'supervisor_title' to impute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated counts for 'gender':\n",
      "gender\n",
      "Female         369572\n",
      "Undisclosed    279413\n",
      "Male            83273\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated counts for 'ethnicity':\n",
      "ethnicity\n",
      "Undisclosed                         318187\n",
      "White                               281480\n",
      "Black/African American               51520\n",
      "Hispanic/White Latino                20605\n",
      "Asian                                19266\n",
      "Hispanic/Other Latino                12929\n",
      "Hispanic                              9999\n",
      "Multicultural/Other                   8977\n",
      "Other                                 3678\n",
      "American Indian/Alaskan Native        3120\n",
      "Native Hawaiian/Pacific Islander      1316\n",
      "Asian/Pacific-Islander                1181\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated counts for 'education':\n",
      "education\n",
      "Not Identified               297805\n",
      "Bachelor's Degree/ non-HR    112208\n",
      "Bachelor's Degree/ HR         53244\n",
      "Master's Degree/ HR           48607\n",
      "Bachelor's Degree             34086\n",
      "Master's Degree/ non-HR       33647\n",
      "High School / GED             32203\n",
      "Associate's Degree            31981\n",
      "MBA/ non-HR Concentration     17776\n",
      "MBA/ HR Concentration         15218\n",
      "Some College                  13241\n",
      "Master's Degree               13037\n",
      "JD                             9364\n",
      "Doctorate                      8445\n",
      "MBA                            8144\n",
      "College Beyond Bachelor        3252\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated counts for 'native_language':\n",
      "native_language\n",
      "Undisclosed                  482607\n",
      "English                      242998\n",
      "Spanish                        3608\n",
      "Arabic                          504\n",
      "Hindustani / Hindi              307\n",
      "French                          251\n",
      "Portuguese                      241\n",
      "Chinese - Mandarin              194\n",
      "Other                           165\n",
      "Japanese                        156\n",
      "German                          155\n",
      "Vietnamese                      124\n",
      "Russian                         112\n",
      "Korean                          105\n",
      "Chinese - Cantonese              75\n",
      "Greek                            60\n",
      "Italian                          56\n",
      "Romanian                         49\n",
      "Egyptian                         48\n",
      "Ukrainian                        41\n",
      "Turkish                          41\n",
      "Polish                           39\n",
      "Afrikaans                        37\n",
      "Finnish                          34\n",
      "Indonesian                       31\n",
      "Thai                             29\n",
      "Bulgarian                        27\n",
      "Punjabi                          27\n",
      "Cantonese                        15\n",
      "Czech                            15\n",
      "Serbian                          14\n",
      "Hebrew                           14\n",
      "Croatian                         13\n",
      "Swedish                          12\n",
      "Sign Language (all forms)        11\n",
      "Danish                           10\n",
      "Hungarian                         9\n",
      "Icelandic                         7\n",
      "Viennese                          6\n",
      "Norwegian                         6\n",
      "Welsh                             3\n",
      "Sudanese                          1\n",
      "Braille                           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Imputation for gender, ethnicity, educationimport pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame containing the data\n",
    "\n",
    "# Replace missing values with 'Unknown' in specified columns\n",
    "final_df['gender'] = final_df['gender'].fillna('Undisclosed')\n",
    "final_df['ethnicity'] = final_df['ethnicity'].fillna('Undisclosed')\n",
    "final_df['education'] = final_df['education'].fillna('Not Identified')\n",
    "final_df['native_language'] = final_df['native_language'].fillna('Undisclosed')\n",
    "\n",
    "# Check the changes\n",
    "print(\"Updated counts for 'gender':\")\n",
    "print(final_df['gender'].value_counts())\n",
    "\n",
    "print(\"\\nUpdated counts for 'ethnicity':\")\n",
    "print(final_df['ethnicity'].value_counts())\n",
    "\n",
    "print(\"\\nUpdated counts for 'education':\")\n",
    "print(final_df['education'].value_counts())\n",
    "\n",
    "print(\"\\nUpdated counts for 'native_language':\")\n",
    "print(final_df['native_language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/1244724403.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  final_df['SHRM_Join_Date'] = pd.to_datetime(final_df['SHRM_Join_Date']).fillna(join_date_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of imputed SHRM_ENTERED_PROFESSION values:\n",
      "8     2015.0\n",
      "14    2015.0\n",
      "18    2012.0\n",
      "19    2012.0\n",
      "20    2019.0\n",
      "Name: SHRM_ENTERED_PROFESSION, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/1244724403.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['SHRM_ENTERED_PROFESSION'] = encoder_profession.inverse_transform(predicted)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "join_date_mode = final_df['SHRM_Join_Date'].dropna().mode()[0]\n",
    "final_df['SHRM_Join_Date'] = pd.to_datetime(final_df['SHRM_Join_Date']).fillna(join_date_mode)\n",
    "\n",
    "# Convert SHRM_Join_Date to an integer: number of days from a specific date (e.g., the mode date)\n",
    "final_df['SHRM_Join_Date_int'] = (final_df['SHRM_Join_Date'] - datetime(2000, 1, 1)).dt.days\n",
    "\n",
    "# Encode the 'standard_job_title' column\n",
    "encoder_job_title = LabelEncoder()\n",
    "final_df['Stadard_Job_Title_encoded'] = encoder_job_title.fit_transform(final_df['Stadard_Job_Title'].astype(str))\n",
    "\n",
    "# Prepare the missing column, assume 'SHRM_ENTERED_PROFESSION' is categorical\n",
    "final_df['SHRM_ENTERED_PROFESSION'] = final_df['SHRM_ENTERED_PROFESSION'].astype(str)  # Ensure data type is consistent\n",
    "encoder_profession = LabelEncoder()\n",
    "final_df['SHRM_ENTERED_PROFESSION_encoded'] = encoder_profession.fit_transform(final_df['SHRM_ENTERED_PROFESSION'])\n",
    "\n",
    "# Separate the data into sets with and without missing values\n",
    "is_missing = final_df['SHRM_ENTERED_PROFESSION'] == 'nan'\n",
    "train_df = final_df[~is_missing]  # non-missing data\n",
    "test_df = final_df[is_missing]    # missing data\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(train_df[['Stadard_Job_Title_encoded', 'SHRM_Join_Date_int']], train_df['SHRM_ENTERED_PROFESSION_encoded'])\n",
    "\n",
    "# Predict the missing values\n",
    "predicted = model.predict(test_df[['Stadard_Job_Title_encoded', 'SHRM_Join_Date_int']])\n",
    "\n",
    "# Inverse transform the predictions back to original labels\n",
    "test_df['SHRM_ENTERED_PROFESSION'] = encoder_profession.inverse_transform(predicted)\n",
    "\n",
    "# Replace the missing values in the original dataframe\n",
    "final_df.loc[is_missing, 'SHRM_ENTERED_PROFESSION'] = test_df['SHRM_ENTERED_PROFESSION']\n",
    "\n",
    "# Optionally, verify and print some of the imputed values\n",
    "print(\"Sample of imputed SHRM_ENTERED_PROFESSION values:\")\n",
    "print(final_df.loc[is_missing, 'SHRM_ENTERED_PROFESSION'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns after dropping:\n",
      "Index(['Customer_ID', 'cert_type', 'Stadard_Job_Title', 'job_function',\n",
      "       'job_title_freetext', 'companyname', 'email',\n",
      "       'DISALLOW_ALL_COMMUNICATION', 'disallow_email_communication',\n",
      "       'disallow_phone_communication', 'disallow_regular_mail_communi',\n",
      "       'disallow_third_party_communic', 'membership_paid_through',\n",
      "       'department_size', 'gender', 'ethnicity', 'education', 'company_size',\n",
      "       'SHRM_ENTERED_PROFESSION', 'shrm_birth_year', 'native_language',\n",
      "       'organization_unit', 'organization_type', 'employee_oversee',\n",
      "       'supervisor_title', 'shrm_organization_multination',\n",
      "       'shrm_organization_unionized', 'SHRM_Join_Date', 'Subsidiary',\n",
      "       'billaddr_1', 'billaddr_city', 'billaddr_state', 'billaddr_zip',\n",
      "       'billaddr_country', 'billaddr_country_desc', 'Current_Start_Date',\n",
      "       'Is_Person', 'IsInActive', 'LoginAccess', 'Monthly_Donor',\n",
      "       'Industry_Category', 'SSO_UUID', 'Customer_Segment', 'targetCertActive',\n",
      "       'Email', 'days_since_cert_start_date', 'cert_start_date_month',\n",
      "       'cert_start_date_weekday', 'SHRM_Join_Date_int',\n",
      "       'Stadard_Job_Title_encoded', 'SHRM_ENTERED_PROFESSION_encoded'],\n",
      "      dtype='object')\n",
      "\n",
      "Dropped columns:\n",
      "Index(['cert_start_date', 'cert_end_date', 'employment_status', 'billaddr_2',\n",
      "       'billaddr_3', 'Cohort', 'ContactId', 'cobalt_name', 'Address1_Country',\n",
      "       'cobalt_expirationdate', 'cobalt_enddate', 'Status Reason',\n",
      "       'Address1_Line1', 'Address1_City', 'Address1_PostalCode',\n",
      "       'Address1_StateOrProvince', 'Address1_Composite', 'EMailAddress1',\n",
      "       'BirthDate', 'shrm_startdate', 'shrm_InitialCertificationDate',\n",
      "       'shrmadm_certificationbegindate', 'shrmadm_certificationenddate',\n",
      "       'shrm_SHRMMember', 'shrm_currentbusinessindustry',\n",
      "       'shrm_currentbusinessindustryvalue', 'shrm_currentpositiontitle',\n",
      "       'shrm_JobPosition', 'shrm_JobPositionvalue', 'shrm_TimesRecertified',\n",
      "       'shrm_TimesRecertified_Date', 'shrm_TimesRecertified_State',\n",
      "       'shrm_ssouid', 'SHRM_SSOEMail'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the threshold for dropping columns, in this case, 80%\n",
    "threshold = 80.0\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages = final_df.isnull().mean() * 100\n",
    "\n",
    "# Find columns that have missing data above the threshold\n",
    "columns_to_drop = missing_percentages[missing_percentages > threshold].index\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "final_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Print the remaining columns to verify\n",
    "print(\"Remaining columns after dropping:\")\n",
    "print(final_df.columns)\n",
    "print(\"\\nDropped columns:\")\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'cert_type', 'Current_Start_Date','days_since_cert_start_date', 'cert_start_date_month','cert_start_date_weekday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  membership_paid_through  days_since_membership_paid_through  \\\n",
      "0              2024-04-30                               -24.0   \n",
      "1              2019-08-31                             -1728.0   \n",
      "2              2024-12-31                               221.0   \n",
      "3              2020-03-31                             -1515.0   \n",
      "4              2024-11-30                               190.0   \n",
      "\n",
      "   membership_paid_through_month  membership_paid_through_weekday  \n",
      "0                              4                                1  \n",
      "1                              8                                5  \n",
      "2                             12                                1  \n",
      "3                              3                                1  \n",
      "4                             11                                5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/3916047560.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  final_df['membership_paid_through'] = pd.to_datetime(final_df['membership_paid_through'])\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/3916047560.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df['days_since_membership_paid_through'].fillna(36500, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'membership_paid_through' to datetime\n",
    "final_df['membership_paid_through'] = pd.to_datetime(final_df['membership_paid_through'])\n",
    "\n",
    "# Reference date for calculating days since the membership paid through date (example: '2024-01-01')\n",
    "reference_date = pd.to_datetime('2024-05-24')\n",
    "\n",
    "# Calculate days since the membership was paid through\n",
    "final_df['days_since_membership_paid_through'] = (final_df['membership_paid_through']-reference_date).dt.days\n",
    "\n",
    "# Fill missing values for days_since_membership_paid_through with a large constant (assuming non-payment or long time since payment)\n",
    "final_df['days_since_membership_paid_through'].fillna(36500, inplace=True)\n",
    "\n",
    "# Additional temporal features\n",
    "final_df['membership_paid_through_month'] = final_df['membership_paid_through'].dt.month.fillna(0).astype(int)  # 0 for missing dates\n",
    "final_df['membership_paid_through_weekday'] = final_df['membership_paid_through'].dt.weekday.fillna(0).astype(int)  # 0 for missing dates\n",
    "\n",
    "# Print sample to verify the changes\n",
    "print(final_df[['membership_paid_through', 'days_since_membership_paid_through', 'membership_paid_through_month', 'membership_paid_through_weekday']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'Industry_Category' after filling: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fill null values in 'Industry_Category' with 'unknown'\n",
    "final_df['Industry_Category'] = final_df['Industry_Category'].fillna('unknown')\n",
    "\n",
    "# Verify the operation by checking if there are any null values left in 'Industry_Category'\n",
    "print(\"Null values in 'Industry_Category' after filling:\", final_df['Industry_Category'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/2928127789.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  final_df['Current_Start_Date'] = pd.to_datetime(final_df['Current_Start_Date'])\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/2928127789.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df['Current_Start_Date'].fillna(pd.to_datetime('1900-01-01'), inplace=True)  # Change '1900-01-01' as needed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame\n",
    "\n",
    "# Convert 'Current_Start_Date' to datetime\n",
    "final_df['Current_Start_Date'] = pd.to_datetime(final_df['Current_Start_Date'])\n",
    "\n",
    "# Reference date for calculating days since the event (example: today)\n",
    "reference_date = pd.to_datetime('2024-01-01')\n",
    "\n",
    "# Days since the event for those who have a start date\n",
    "final_df['days_since_current_start_date'] = (reference_date - final_df['Current_Start_Date']).dt.days\n",
    "\n",
    "# Fill missing values with a placeholder date\n",
    "final_df['Current_Start_Date'].fillna(pd.to_datetime('1900-01-01'), inplace=True)  # Change '1900-01-01' as needed\n",
    "\n",
    "# Additional temporal features (optional)\n",
    "final_df['current_start_date_month'] = final_df['Current_Start_Date'].dt.month.fillna(0).astype(int)  # 0 for missing values\n",
    "final_df['current_start_date_weekday'] = final_df['Current_Start_Date'].dt.weekday.fillna(0).astype(int)  # 0 for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns after dropping:\n",
      "Index(['Customer_ID', 'cert_type', 'Stadard_Job_Title', 'companyname', 'email',\n",
      "       'DISALLOW_ALL_COMMUNICATION', 'disallow_email_communication',\n",
      "       'disallow_phone_communication', 'disallow_regular_mail_communi',\n",
      "       'disallow_third_party_communic', 'department_size', 'gender',\n",
      "       'ethnicity', 'education', 'company_size', 'SHRM_ENTERED_PROFESSION',\n",
      "       'shrm_birth_year', 'native_language', 'organization_unit',\n",
      "       'organization_type', 'employee_oversee', 'supervisor_title',\n",
      "       'shrm_organization_multination', 'shrm_organization_unionized',\n",
      "       'SHRM_Join_Date', 'Subsidiary', 'billaddr_1', 'billaddr_city',\n",
      "       'billaddr_state', 'billaddr_zip', 'billaddr_country',\n",
      "       'billaddr_country_desc', 'Current_Start_Date', 'Is_Person',\n",
      "       'IsInActive', 'LoginAccess', 'Monthly_Donor', 'Industry_Category',\n",
      "       'Customer_Segment', 'targetCertActive', 'days_since_cert_start_date',\n",
      "       'cert_start_date_month', 'cert_start_date_weekday',\n",
      "       'SHRM_Join_Date_int', 'Stadard_Job_Title_encoded',\n",
      "       'SHRM_ENTERED_PROFESSION_encoded', 'days_since_membership_paid_through',\n",
      "       'membership_paid_through_month', 'membership_paid_through_weekday',\n",
      "       'days_since_current_start_date', 'current_start_date_month',\n",
      "       'current_start_date_weekday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame\n",
    "# Load your DataFrame (uncomment and modify the path if starting from a CSV file)\n",
    "# final_df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "# List of columns to be dropped\n",
    "columns_to_drop = ['job_function', 'job_title_freetext', 'Email', 'SSO_UUID', 'membership_paid_through']\n",
    "\n",
    "# Check and drop only if the column is present in the DataFrame\n",
    "columns_to_drop = [col for col in columns_to_drop if col in final_df.columns]\n",
    "final_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Print the remaining columns to verify\n",
    "print(\"Remaining columns after dropping:\")\n",
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_40415/96620848.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df[column].fillna(mode_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values in billaddr_1 with mode: 1800 Duke St\n",
      "Filled missing values in billaddr_city with mode: New York\n",
      "Filled missing values in billaddr_state with mode: CA\n",
      "Filled missing values in billaddr_zip with mode: 00000\n",
      "Filled missing values in billaddr_country with mode: US\n",
      "Filled missing values in billaddr_country_desc with mode: United States\n",
      "Filled missing values in shrm_organization_multination with mode: F\n",
      "Filled missing values in shrm_organization_unionized with mode: F\n",
      "Filled missing values in shrm_birth_year with mode: 0.0\n",
      "Filled missing values in disallow_phone_communication with mode: F\n",
      "Filled missing values in disallow_third_party_communic with mode: F\n",
      "Filled missing values in Monthly_Donor with mode: F\n",
      "\n",
      "Missing values after filling:\n",
      "billaddr_1: 0 missing values\n",
      "billaddr_city: 0 missing values\n",
      "billaddr_state: 0 missing values\n",
      "billaddr_zip: 0 missing values\n",
      "billaddr_country: 0 missing values\n",
      "billaddr_country_desc: 0 missing values\n",
      "shrm_organization_multination: 0 missing values\n",
      "shrm_organization_unionized: 0 missing values\n",
      "shrm_birth_year: 0 missing values\n",
      "disallow_phone_communication: 0 missing values\n",
      "disallow_third_party_communic: 0 missing values\n",
      "Monthly_Donor: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame\n",
    "# Load your DataFrame (uncomment and modify the path if starting from a CSV file)\n",
    "# final_df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "# List of columns to fill with the mode\n",
    "columns_to_fill = [\n",
    "    'billaddr_1', 'billaddr_city', 'billaddr_state', 'billaddr_zip', \n",
    "    'billaddr_country', 'billaddr_country_desc', 'shrm_organization_multination',\n",
    "    'shrm_organization_unionized', 'shrm_birth_year', 'disallow_phone_communication',\n",
    "    'disallow_third_party_communic', 'Monthly_Donor'\n",
    "]\n",
    "\n",
    "# Loop through each column and fill missing values with the mode\n",
    "for column in columns_to_fill:\n",
    "    # Calculate the mode of the column\n",
    "    mode_value = final_df[column].mode()[0]  # mode() returns a Series; use [0] to get the mode value\n",
    "    \n",
    "    # Fill missing values with the mode\n",
    "    final_df[column].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    # Optionally print the mode used for each column\n",
    "    print(f\"Filled missing values in {column} with mode: {mode_value}\")\n",
    "\n",
    "# Check if any of these columns still have missing values\n",
    "print(\"\\nMissing values after filling:\")\n",
    "for column in columns_to_fill:\n",
    "    print(f\"{column}: {final_df[column].isnull().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in each column:\n",
      "Customer_ID                           0.000000\n",
      "cert_type                             0.000000\n",
      "Stadard_Job_Title                     0.000000\n",
      "companyname                           0.000000\n",
      "email                                 0.000000\n",
      "DISALLOW_ALL_COMMUNICATION            0.000000\n",
      "disallow_email_communication          0.000000\n",
      "disallow_phone_communication          0.000000\n",
      "disallow_regular_mail_communi         0.000000\n",
      "disallow_third_party_communic         0.000000\n",
      "department_size                       0.000000\n",
      "gender                                0.000000\n",
      "ethnicity                             0.000000\n",
      "education                             0.000000\n",
      "company_size                          0.000000\n",
      "SHRM_ENTERED_PROFESSION               0.000000\n",
      "shrm_birth_year                       0.000000\n",
      "native_language                       0.000000\n",
      "organization_unit                     0.000000\n",
      "organization_type                     0.000000\n",
      "employee_oversee                      0.000000\n",
      "supervisor_title                      0.000000\n",
      "shrm_organization_multination         0.000000\n",
      "shrm_organization_unionized           0.000000\n",
      "SHRM_Join_Date                        0.000000\n",
      "Subsidiary                            0.000000\n",
      "billaddr_1                            0.000000\n",
      "billaddr_city                         0.000000\n",
      "billaddr_state                        0.000000\n",
      "billaddr_zip                          0.000000\n",
      "billaddr_country                      0.000000\n",
      "billaddr_country_desc                 0.000000\n",
      "Current_Start_Date                    0.000000\n",
      "Is_Person                             0.000000\n",
      "IsInActive                            0.000000\n",
      "LoginAccess                           0.000000\n",
      "Monthly_Donor                         0.000000\n",
      "Industry_Category                     0.000000\n",
      "Customer_Segment                      0.000000\n",
      "targetCertActive                      0.000000\n",
      "days_since_cert_start_date            0.000000\n",
      "cert_start_date_month                 0.000000\n",
      "cert_start_date_weekday               0.000000\n",
      "SHRM_Join_Date_int                    0.000000\n",
      "Stadard_Job_Title_encoded             0.000000\n",
      "SHRM_ENTERED_PROFESSION_encoded       0.000000\n",
      "days_since_membership_paid_through    0.000000\n",
      "membership_paid_through_month         0.000000\n",
      "membership_paid_through_weekday       0.000000\n",
      "days_since_current_start_date         2.957154\n",
      "current_start_date_month              0.000000\n",
      "current_start_date_weekday            0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame\n",
    "# Set display options to avoid truncation\n",
    "pd.set_option('display.max_rows', None)  # Adjust as necessary for row display\n",
    "pd.set_option('display.max_columns', None)  # Adjust as necessary for column display\n",
    "pd.set_option('display.width', None)  # Adjusts the width of the display to show entire columns\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each column is shown\n",
    "\n",
    "# Check the percentage of missing values in each column\n",
    "missing_data = final_df.isnull().mean() * 100\n",
    "print(\"Percentage of missing values in each column:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Column Name Null Count Distinct Count Total Count       Data Type\n",
      "0                          Customer_ID          0         474692      732258         float64\n",
      "1                            cert_type          0              3      732258          object\n",
      "2                    Stadard_Job_Title          0             47      732258          object\n",
      "3                          companyname          0         381931      732258          object\n",
      "4                                email          0         731091      732258          object\n",
      "5           DISALLOW_ALL_COMMUNICATION          0              2      732258          object\n",
      "6         disallow_email_communication          0              2      732258          object\n",
      "7         disallow_phone_communication          0              2      732258          object\n",
      "8        disallow_regular_mail_communi          0              2      732258          object\n",
      "9        disallow_third_party_communic          0              2      732258          object\n",
      "10                     department_size          0              7      732258           int64\n",
      "11                              gender          0              3      732258          object\n",
      "12                           ethnicity          0             12      732258          object\n",
      "13                           education          0             16      732258          object\n",
      "14                        company_size          0             11      732258           int64\n",
      "15             SHRM_ENTERED_PROFESSION          0             93      732258          object\n",
      "16                     shrm_birth_year          0            127      732258         float64\n",
      "17                     native_language          0             43      732258          object\n",
      "18                   organization_unit          0              7      732258           int64\n",
      "19                   organization_type          0              6      732258           int64\n",
      "20                    employee_oversee          0              9      732258          object\n",
      "21                    supervisor_title          0             16      732258          object\n",
      "22       shrm_organization_multination          0              2      732258          object\n",
      "23         shrm_organization_unionized          0              2      732258          object\n",
      "24                      SHRM_Join_Date          0          10750      732258  datetime64[ns]\n",
      "25                          Subsidiary          0              2      732258          object\n",
      "26                          billaddr_1          0         565581      732258          object\n",
      "27                       billaddr_city          0          25241      732258          object\n",
      "28                      billaddr_state          0           2859      732258          object\n",
      "29                        billaddr_zip          0         391153      732258          object\n",
      "30                    billaddr_country          0            208      732258          object\n",
      "31               billaddr_country_desc          0            209      732258          object\n",
      "32                  Current_Start_Date          0           2604      732258  datetime64[ns]\n",
      "33                           Is_Person          0              2      732258          object\n",
      "34                          IsInActive          0              2      732258          object\n",
      "35                         LoginAccess          0              2      732258          object\n",
      "36                       Monthly_Donor          0              2      732258          object\n",
      "37                   Industry_Category          0             33      732258          object\n",
      "38                    Customer_Segment          0              6      732258          object\n",
      "39                    targetCertActive          0              2      732258          object\n",
      "40          days_since_cert_start_date          0           1738      732258         float64\n",
      "41               cert_start_date_month          0             13      732258           int64\n",
      "42             cert_start_date_weekday          0              7      732258           int64\n",
      "43                  SHRM_Join_Date_int          0          10750      732258           int64\n",
      "44           Stadard_Job_Title_encoded          0             47      732258           int64\n",
      "45     SHRM_ENTERED_PROFESSION_encoded          0             94      732258           int64\n",
      "46  days_since_membership_paid_through          0           1140      732258         float64\n",
      "47       membership_paid_through_month          0             13      732258           int64\n",
      "48     membership_paid_through_weekday          0              7      732258           int64\n",
      "49       days_since_current_start_date      21654           2603      732258         float64\n",
      "50            current_start_date_month          0             12      732258           int64\n",
      "51          current_start_date_weekday          0              7      732258           int64\n"
     ]
    }
   ],
   "source": [
    "#Decide Imputation strategy\n",
    "df = final_df\n",
    "# Create a DataFrame to hold the summary\n",
    "summary_df = pd.DataFrame(columns=['Column Name', 'Null Count', 'Distinct Count', 'Total Count', 'Data Type'])\n",
    "\n",
    "# Iterate over each column in the original DataFrame\n",
    "for column in df.columns:\n",
    "    # Create a temporary DataFrame for current column statistics\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Column Name': [column],\n",
    "        'Null Count': [df[column].isnull().sum()],\n",
    "        'Distinct Count': [df[column].nunique()],\n",
    "        'Total Count': [df[column].size],\n",
    "        'Data Type': [df[column].dtype]\n",
    "    })\n",
    "    # Concatenate the temporary DataFrame to the summary DataFrame\n",
    "    summary_df = pd.concat([summary_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Adjust pandas display settings\n",
    "pd.set_option('display.max_rows', None)  # Adjust to show all rows\n",
    "pd.set_option('display.max_columns', None)  # Adjust to show all columns\n",
    "pd.set_option('display.width', 1000)  # Adjust the width to fit your screen\n",
    "pd.set_option('display.max_colwidth', None)  # Adjust to show full content of each column\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DISALLOW_ALL_COMMUNICATION', 'disallow_email_communication', 'disallow_phone_communication', 'disallow_regular_mail_communi', 'disallow_third_party_communic', 'department_size', 'gender', 'ethnicity', 'education', 'company_size', 'shrm_birth_year', 'native_language', 'organization_unit', 'organization_type', 'employee_oversee', 'supervisor_title', 'shrm_organization_multination', 'shrm_organization_unionized', 'Subsidiary', 'billaddr_city', 'billaddr_state', 'billaddr_zip', 'billaddr_country', 'billaddr_country_desc', 'Is_Person', 'IsInActive', 'LoginAccess', 'Monthly_Donor', 'Industry_Category', 'Customer_Segment', 'targetCertActive', 'SHRM_Join_Date_int', 'Stadard_Job_Title_encoded', 'SHRM_ENTERED_PROFESSION_encoded', 'days_since_membership_paid_through', 'membership_paid_through_month', 'membership_paid_through_weekday'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = final_df\n",
    "df = df.drop(['companyname', 'Stadard_Job_Title', 'SHRM_ENTERED_PROFESSION', 'cert_start_date_month', 'cert_start_date_weekday','Customer_ID', 'email', 'SHRM_Join_Date', 'Current_Start_Date', 'cert_type', 'days_since_current_start_date', 'billaddr_1', 'current_start_date_month', 'current_start_date_weekday','days_since_cert_start_date'], axis=1, errors='ignore')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DISALLOW_ALL_COMMUNICATION', 'disallow_email_communication', 'disallow_phone_communication', 'disallow_regular_mail_communi', 'disallow_third_party_communic', 'department_size', 'gender', 'ethnicity', 'education', 'company_size', 'shrm_birth_year', 'native_language', 'organization_unit', 'organization_type', 'employee_oversee', 'supervisor_title', 'shrm_organization_multination', 'shrm_organization_unionized', 'Subsidiary', 'billaddr_city', 'billaddr_state', 'billaddr_zip', 'billaddr_country', 'billaddr_country_desc', 'Is_Person', 'IsInActive', 'LoginAccess', 'Monthly_Donor', 'Industry_Category', 'Customer_Segment', 'targetCertActive', 'SHRM_Join_Date_int', 'Stadard_Job_Title_encoded', 'SHRM_ENTERED_PROFESSION_encoded', 'days_since_membership_paid_through', 'membership_paid_through_month', 'membership_paid_through_weekday'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary encoding complete\n",
      "One-Hot encoding complete\n",
      "Target encoding complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, recall_score\n",
    "import category_encoders as ce\n",
    "\n",
    "# Sample DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')  # Ensure df is loaded with your dataset\n",
    "\n",
    "# Encode the target variable\n",
    "df['targetCertActive'] = df['targetCertActive'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Initialize dictionaries to store mappings\n",
    "binary_mappings = {}\n",
    "one_hot_mappings = {}\n",
    "target_mappings = {}\n",
    "\n",
    "# Binary Encoding\n",
    "binary_cols = [\n",
    "    'DISALLOW_ALL_COMMUNICATION',\n",
    "    'disallow_email_communication',\n",
    "    'disallow_phone_communication',\n",
    "    'disallow_regular_mail_communi',\n",
    "    'disallow_third_party_communic',\n",
    "    'shrm_organization_multination',\n",
    "    'shrm_organization_unionized',\n",
    "    'Subsidiary',\n",
    "    'Is_Person',\n",
    "    'IsInActive',\n",
    "    'LoginAccess',\n",
    "    'Monthly_Donor'\n",
    "]\n",
    "\n",
    "# Create mappings and encode binary columns\n",
    "for col in binary_cols:\n",
    "    mapping = {'Yes': 1, 'No': 0, True: 1, False: 0, pd.NA: -1, None: -1}\n",
    "    binary_mappings[col] = mapping\n",
    "    df[col] = df[col].map(mapping).fillna(-1)\n",
    "\n",
    "print(\"Binary encoding complete\")\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_cols = [\n",
    "    #'Stadard_Job_Title',\n",
    "    'gender',\n",
    "    'ethnicity',\n",
    "    'education',\n",
    "   # 'SHRM_ENTERED_PROFESSION',\n",
    "    'native_language',\n",
    "    'employee_oversee',\n",
    "    'supervisor_title',\n",
    "    'Industry_Category',\n",
    "    'Customer_Segment'\n",
    "]\n",
    "\n",
    "# Store original categories\n",
    "for col in one_hot_cols:\n",
    "    one_hot_mappings[col] = list(df[col].unique())\n",
    "\n",
    "df = pd.get_dummies(df, columns=one_hot_cols)\n",
    "print(\"One-Hot encoding complete\")\n",
    "\n",
    "# Target Encoding\n",
    "target_encode_cols = [\n",
    "    #'companyname',\n",
    "    'billaddr_city',\n",
    "    'billaddr_state',\n",
    "    'billaddr_zip',\n",
    "    'billaddr_country',\n",
    "    'billaddr_country_desc'\n",
    "]\n",
    "\n",
    "# Store mappings for target encoding\n",
    "target_encoder = ce.TargetEncoder(cols=target_encode_cols)\n",
    "df[target_encode_cols] = target_encoder.fit_transform(df[target_encode_cols], df['targetCertActive'])\n",
    "\n",
    "for col in target_encode_cols:\n",
    "    target_mappings[col] = target_encoder.mapping[col]\n",
    "\n",
    "print(\"Target encoding complete\")\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('targetCertActive', axis=1)\n",
    "y = df['targetCertActive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'final_df' is your DataFrame\n",
    "# Set display options to avoid truncation\n",
    "pd.set_option('display.max_rows', None)  # Adjust as necessary for row display\n",
    "pd.set_option('display.max_columns', None)  # Adjust as necessary for column display\n",
    "pd.set_option('display.width', None)  # Adjusts the width of the display to show entire columns\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each column is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.93345965 0.93303232 0.93296404 0.93350176 0.93140209]\n",
      "Mean Cross-Validation Score: 0.9328719736778378\n",
      "Train Accuracy: 0.9990474662260203\n",
      "Test Accuracy: 0.9325717641274957\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96    120088\n",
      "           1       0.88      0.73      0.80     26364\n",
      "\n",
      "    accuracy                           0.93    146452\n",
      "   macro avg       0.91      0.85      0.88    146452\n",
      "weighted avg       0.93      0.93      0.93    146452\n",
      "\n",
      "Confusion Matrix:\n",
      "[[117358   2730]\n",
      " [  7145  19219]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('targetCertActive', axis=1)\n",
    "y = df['targetCertActive']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Score:\", cv_scores.mean())\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "classification_rep = classification_report(y_test, y_pred_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** AUC = 97.13%    Recall = 72.90%  ***** \n",
      "\n",
      "   Threshold    AUC: Recall: Target %             Confusion Matrix\n",
      "0     0.5000  85.31%  72.90%  14.987%   [117358,2730],[7145,19219]\n",
      "1     0.4000  88.53%  81.08%  17.894%   [115257,4831],[4989,21375]\n",
      "2     0.3000  90.74%  87.86%  21.044%   [112432,7656],[3200,23164]\n",
      "3     0.2000  91.61%  92.91%  24.678%  [108442,11646],[1869,24495]\n",
      "4     0.1000  89.25%  97.14%  32.779%    [97693,22395],[753,25611]\n",
      "5     0.0100  70.08%  99.54%  66.604%    [48788,71300],[121,26243]\n",
      "6     0.0050  64.18%  99.75%  76.489%     [34366,85722],[67,26297]\n",
      "7     0.0010  64.15%  99.75%  76.544%     [34285,85803],[67,26297]\n",
      "8     0.0001  64.15%  99.75%  76.548%     [34279,85809],[67,26297]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    \"\"\"Evaluate the model and print metrics.\"\"\"\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    propensity_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Generate binary predictions based on a default threshold of 0.5\n",
    "    predictions = (propensity_scores > 0.5).astype(int)\n",
    "\n",
    "    # Calculate AUC using the probability scores\n",
    "    y_test = y_test.astype(int)\n",
    "    auc_score = roc_auc_score(y_test, propensity_scores)\n",
    "\n",
    "    # Calculate recall using the binary predictions\n",
    "    recall = recall_score(y_test, predictions)\n",
    "\n",
    "    # Print the basic evaluation metrics\n",
    "    print(f\"***** AUC = {auc_score:.2%}    Recall = {recall:.2%}  ***** \\n\")\n",
    "\n",
    "    # Threshold analysis\n",
    "    thresholds = [0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.005, 0.001, 0.0001]\n",
    "    auc_list, recall_list, target_list, confusion_matrices = [], [], [], []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Generate binary predictions based on the current threshold\n",
    "        y_pred_threshold = (propensity_scores > threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics for the current threshold\n",
    "        auc_list.append(roc_auc_score(y_test, y_pred_threshold))\n",
    "        recall_list.append(recall_score(y_test, y_pred_threshold))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n",
    "        target_list.append((fp + tp) / len(y_test))\n",
    "        confusion_matrices.append(f\"[{tn},{fp}],[{fn},{tp}]\")\n",
    "\n",
    "    # Prepare and print the summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Threshold': thresholds,\n",
    "        'AUC:': [f\"{x:.2%}\" for x in auc_list],\n",
    "        'Recall:': [f\"{x:.2%}\" for x in recall_list],\n",
    "        'Target %': [f\"{x:.3%}\" for x in target_list],\n",
    "        'Confusion Matrix': confusion_matrices\n",
    "    })\n",
    "    print(summary_table)\n",
    "\n",
    "def get_feature_importances(clf, features):\n",
    "    \"\"\"Get feature importances from the trained model.\"\"\"\n",
    "    importances = clf.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    return np.array(features)[sorted_idx], importances[sorted_idx]\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('targetCertActive', axis=1)\n",
    "y = df['targetCertActive']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using the custom function\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
